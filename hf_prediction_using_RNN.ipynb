{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import DataLoader, Dataset\r\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import random\r\n",
        "import pickle\r\n",
        "\r\n",
        "# Set seed\r\n",
        "seed = 24\r\n",
        "random.seed(seed)\r\n",
        "np.random.seed(seed)\r\n",
        "torch.manual_seed(seed)\r\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\r\n",
        "\r\n",
        "# Define data path\r\n",
        "DATA_PATH = \"../HW3_RNN-lib/data\"\r\n",
        "\r\n",
        "# Load data\r\n",
        "pids = pickle.load(open(os.path.join(DATA_PATH, 'train/pids.pkl'), 'rb'))\r\n",
        "vids = pickle.load(open(os.path.join(DATA_PATH, 'train/vids.pkl'), 'rb'))\r\n",
        "hfs = pickle.load(open(os.path.join(DATA_PATH, 'train/hfs.pkl'), 'rb'))\r\n",
        "seqs = pickle.load(open(os.path.join(DATA_PATH, 'train/seqs.pkl'), 'rb'))\r\n",
        "types = pickle.load(open(os.path.join(DATA_PATH, 'train/types.pkl'), 'rb'))\r\n",
        "rtypes = pickle.load(open(os.path.join(DATA_PATH, 'train/rtypes.pkl'), 'rb'))\r\n",
        "\r\n",
        "input_size = len(types)\r\n",
        "\r\n",
        "def pad_and_encode_sequence(sequence, max_visits, max_seq_length):\r\n",
        "    padded_sequence = np.zeros((max_visits, max_seq_length))\r\n",
        "    for i, visit in enumerate(sequence[:max_visits]):\r\n",
        "        for code in visit:\r\n",
        "            padded_sequence[i][code] = 1\r\n",
        "    return padded_sequence\r\n",
        "\r\n",
        "# Preprocess the data for 12-month observation window\r\n",
        "max_visits_12 = 12\r\n",
        "seqs_12 = [pad_and_encode_sequence(seq, max_visits_12, input_size) for seq in seqs]\r\n",
        "seqs_12 = np.array(seqs_12)\r\n",
        "\r\n",
        "# Preprocess the data for 18-month observation window\r\n",
        "max_visits_18 = 18\r\n",
        "seqs_18 = [pad_and_encode_sequence(seq, max_visits_18, input_size) for seq in seqs]\r\n",
        "seqs_18 = np.array(seqs_18)\r\n",
        "\r\n",
        "def train_and_evaluate_models(train_seqs, test_seqs, train_hfs, test_hfs):\r\n",
        "    class RNNModel(nn.Module):\r\n",
        "        def __init__(self, input_size, hidden_size, output_size):\r\n",
        "            super(RNNModel, self).__init__()\r\n",
        "            self.hidden_size = hidden_size\r\n",
        "            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\r\n",
        "            self.fc = nn.Linear(hidden_size, output_size)\r\n",
        "\r\n",
        "        def forward(self, x):\r\n",
        "            h0 = torch.zeros(1, x.size(0), self.hidden_size)\r\n",
        "            out, _ = self.rnn(x, h0)\r\n",
        "            out = self.fc(out[:, -1, :])\r\n",
        "            return out\r\n",
        "\r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "    hidden_size = 128\r\n",
        "    output_size = 1\r\n",
        "    learning_rate = 0.001\r\n",
        "    num_epochs = 100\r\n",
        "\r\n",
        "    rnn_model = RNNModel(input_size, hidden_size, output_size).to(device)\r\n",
        "    criterion = nn.BCEWithLogitsLoss()\r\n",
        "    optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "    train_seqs_tensor = torch.tensor(train_seqs, dtype=torch.float).to(device)\r\n",
        "    train_hfs_tensor = torch.tensor(train_hfs, dtype=torch.float).unsqueeze(1).to(device)\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        rnn_model.train()\r\n",
        "        optimizer.zero_grad()\r\n",
        "        predictions = rnn_model(train_seqs_tensor)\r\n",
        "        loss = criterion(predictions, train_hfs_tensor)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "    lr_model = LogisticRegression()\r\n",
        "    lr_model.fit(train_seqs.reshape(len(train_seqs), -1), train_hfs)\r\n",
        "\r\n",
        "    mlp_model = MLPClassifier(hidden_layer_sizes=(128,), random_state=seed)\r\n",
        "    mlp_model.fit(train_seqs.reshape(len(train_seqs), -1), train_hfs)\r\n",
        "\r\n",
        "    svm_model = SVC(probability=True, random_state=seed)\r\n",
        "    svm_model.fit(train_seqs.reshape(len(train_seqs), -1), train_hfs)\r\n",
        "\r\n",
        "    knn_model = KNeighborsClassifier()\r\n",
        "    knn_model.fit(train_seqs.reshape(len(train_seqs), -1), train_hfs)\r\n",
        "\r\n",
        "    test_seqs_tensor = torch.tensor(test_seqs, dtype=torch.float).to(device)\r\n",
        "    rnn_auc = roc_auc_score(test_hfs, torch.sigmoid(rnn_model(test_seqs_tensor)).detach().numpy())\r\n",
        "    lr_auc = roc_auc_score(test_hfs, lr_model.predict_proba(test_seqs.reshape(len(test_seqs), -1))[:, 1])\r\n",
        "    mlp_auc = roc_auc_score(test_hfs, mlp_model.predict_proba(test_seqs.reshape(len(test_seqs), -1))[:, 1])\r\n",
        "    svm_auc = roc_auc_score(test_hfs, svm_model.predict_proba(test_seqs.reshape(len(test_seqs), -1))[:, 1])\r\n",
        "    knn_auc = roc_auc_score(test_hfs, knn_model.predict_proba(test_seqs.reshape(len(test_seqs), -1))[:, 1])\r\n",
        "\r\n",
        "    return rnn_auc, lr_auc, mlp_auc, svm_auc, knn_auc\r\n",
        "\r\n",
        "train_seqs_12, test_seqs_12, train_hfs_12, test_hfs_12 = train_test_split(seqs_12, hfs, test_size=0.2, random_state=seed)\r\n",
        "train_seqs_18, test_seqs_18, train_hfs_18, test_hfs_18 = train_test_split(seqs_18, hfs, test_size=0.2, random_state=seed)\r\n",
        "\r\n",
        "aucs_12 = train_and_evaluate_models(train_seqs_12, test_seqs_12, train_hfs_12, test_hfs_12)\r\n",
        "print(\"Results for 12-month observation window:\")\r\n",
        "print(f\"RNN AUC: {aucs_12[0]:.3f}\")\r\n",
        "print(f\"Logistic Regression AUC: {aucs_12[1]:.3f}\")\r\n",
        "print(f\"MLP AUC: {aucs_12[2]:.3f}\")\r\n",
        "print(f\"SVM AUC: {aucs_12[3]:.3f}\")\r\n",
        "print(f\"KNN AUC: {aucs_12[4]:.3f}\")\r\n",
        "\r\n",
        "aucs_18 = train_and_evaluate_models(train_seqs_18, test_seqs_18, train_hfs_18, test_hfs_18)\r\n",
        "print(\"Results for 18-month observation window:\")\r\n",
        "print(f\"RNN AUC: {aucs_18[0]:.3f}\")\r\n",
        "print(f\"Logistic Regression AUC: {aucs_18[1]:.3f}\")\r\n",
        "print(f\"MLP AUC: {aucs_18[2]:.3f}\")\r\n",
        "print(f\"SVM AUC: {aucs_18[3]:.3f}\")\r\n",
        "print(f\"KNN AUC: {aucs_18[4]:.3f}\")\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}